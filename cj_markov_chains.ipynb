{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake dataset 1 - prosty graf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake1 = pd.DataFrame([\n",
    "     {'prev':'START', 'next':'A', 'prob':0.3}\n",
    "    ,{'prev':'START', 'next':'B', 'prob':0.7}\n",
    "    ,{'prev':'A', 'next':'C', 'prob':0.2}\n",
    "    ,{'prev':'A', 'next':'D', 'prob':0.8}\n",
    "    ,{'prev':'B', 'next':'D', 'prob':1.0}\n",
    "    ,{'prev':'C', 'next':'N_CONV', 'prob':1.0}\n",
    "    ,{'prev':'D', 'next':'N_CONV', 'prob':0.1}\n",
    "    ,{'prev':'D', 'next':'CONV', 'prob':0.9}\n",
    "])\n",
    "\n",
    "print(fake1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    graph = pd.DataFrame()\n",
    "    active_crawlers = []\n",
    "    paths = []\n",
    "    current_index = 0\n",
    "    \n",
    "    def __init__(self, history):\n",
    "        if Crawler.graph.empty:\n",
    "            print('Before using Crawler you must load a graph! (Crawler.load_graph())')\n",
    "        else:\n",
    "            Crawler.active_crawlers.append(self)\n",
    "            self.index = Crawler.current_index\n",
    "            Crawler.current_index += 1\n",
    "            self.history = history\n",
    "            print('Created crawler{}, active crawlers: {} ({})'.format(self.index, len(Crawler.active_crawlers), self.history))\n",
    "    \n",
    "    def _step(self):\n",
    "        current_node = self.history[-1]\n",
    "        exits = Crawler.graph[Crawler.graph['prev'] == current_node]['next']\n",
    "        if exits.empty:\n",
    "            print('\\t\\tCrawler{} finished'.format(self.index))\n",
    "            print('\\t\\tCrawler{}\\'s path: {} '.format(self.index, self.history))\n",
    "            if current_node in ['CONV', 'N_CONV']:\n",
    "                Crawler.paths.append(self.history)\n",
    "        else:\n",
    "            for e in exits:\n",
    "                new = Crawler(self.history.copy())\n",
    "                new.history.append(e)\n",
    "                new._step()\n",
    "        idx = self.index\n",
    "        Crawler.active_crawlers.remove(self)\n",
    "        print('Removed crawler{}, active crawlers: {}'.format(idx, len(Crawler.active_crawlers)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def start():\n",
    "        c0 = Crawler(['START'])\n",
    "        c0._step()\n",
    "            \n",
    "    @staticmethod\n",
    "    def load_graph(graph):\n",
    "        Crawler.graph = graph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crawler0, active crawlers: 1 (['START'])\n",
      "Created crawler1, active crawlers: 2 (['START'])\n",
      "Created crawler2, active crawlers: 3 (['START', 'A'])\n",
      "Created crawler3, active crawlers: 4 (['START', 'A', 'C'])\n",
      "\t\tCrawler3 finished\n",
      "\t\tCrawler3's path: ['START', 'A', 'C', 'N_CONV'] \n",
      "Removed crawler3, active crawlers: 3\n",
      "Removed crawler2, active crawlers: 2\n",
      "Created crawler4, active crawlers: 3 (['START', 'A'])\n",
      "Created crawler5, active crawlers: 4 (['START', 'A', 'D'])\n",
      "\t\tCrawler5 finished\n",
      "\t\tCrawler5's path: ['START', 'A', 'D', 'N_CONV'] \n",
      "Removed crawler5, active crawlers: 3\n",
      "Created crawler6, active crawlers: 4 (['START', 'A', 'D'])\n",
      "\t\tCrawler6 finished\n",
      "\t\tCrawler6's path: ['START', 'A', 'D', 'CONV'] \n",
      "Removed crawler6, active crawlers: 3\n",
      "Removed crawler4, active crawlers: 2\n",
      "Removed crawler1, active crawlers: 1\n",
      "Created crawler7, active crawlers: 2 (['START'])\n",
      "Created crawler8, active crawlers: 3 (['START', 'B'])\n",
      "Created crawler9, active crawlers: 4 (['START', 'B', 'D'])\n",
      "\t\tCrawler9 finished\n",
      "\t\tCrawler9's path: ['START', 'B', 'D', 'N_CONV'] \n",
      "Removed crawler9, active crawlers: 3\n",
      "Created crawler10, active crawlers: 4 (['START', 'B', 'D'])\n",
      "\t\tCrawler10 finished\n",
      "\t\tCrawler10's path: ['START', 'B', 'D', 'CONV'] \n",
      "Removed crawler10, active crawlers: 3\n",
      "Removed crawler8, active crawlers: 2\n",
      "Removed crawler7, active crawlers: 1\n",
      "Removed crawler0, active crawlers: 0\n"
     ]
    }
   ],
   "source": [
    "Crawler.load_graph(fake1)\n",
    "Crawler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['START', 'A', 'C', 'N_CONV'],\n",
       " ['START', 'A', 'D', 'N_CONV'],\n",
       " ['START', 'A', 'D', 'CONV'],\n",
       " ['START', 'B', 'D', 'N_CONV'],\n",
       " ['START', 'B', 'D', 'CONV']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crawler.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
